{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Import the best device available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Load datasets\n",
    "raw_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_function,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Training function. Loads the batch, perform forward pass, compute gradients and perform backward pass.\n",
    "    Only difference is that you can pass the loss function as an argument.\n",
    "    This allows to use a simple cross entropy loss function, or a more complex one including L2 or EWC regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        # move data and target to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # do the forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        # compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # perform the gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(train_dataloader)\n",
    "\n",
    "def validate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    val_dataloader: DataLoader,\n",
    "    loss_function,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Validates the model on the validation split.\n",
    "    \"\"\"\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_dataloader):\n",
    "            # move data and target to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # do the forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = loss_function(output, target)\n",
    "\n",
    "            # print statistics\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_dataloader)\n",
    "\n",
    "def fit(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    The fit method calls the train_epoch() method for a specified\n",
    "    number of epochs and returns the train and validation losses.\n",
    "    \"\"\"\n",
    "\n",
    "    # keep track of the losses in order to visualize them later\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        t = time() # current time\n",
    "\n",
    "        # train function\n",
    "        train_loss = train_epoch(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # validate model\n",
    "        val_loss = validate_epoch(\n",
    "            model=model,\n",
    "            val_dataloader=val_dataloader,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # step scheduler if needed\n",
    "        if scheduler != None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # append losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        dt = time() - t # time difference\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]: train={round(train_loss, 4)} val={round(val_loss, 4)} ({round(dt, 1)}s)\")\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:  996376\n"
     ]
    }
   ],
   "source": [
    "class SmallNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "\n",
    "        self._n_conv1 = 32\n",
    "        self._n_conv2 = 64\n",
    "        self._n_conv3 = 128\n",
    "        self._n_ff = 420\n",
    "        self._n_classes = 100\n",
    "        self._drop_conv = 0.1\n",
    "        self._drop_dense = 0.2\n",
    "        \n",
    "        self._flattened_size = self._n_conv3 * 4 * 4 # !! number of pooling layers !!\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, self._n_conv1, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(self._n_conv1)\n",
    "        self.do1 = torch.nn.Dropout(self._drop_conv)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(self._n_conv1, self._n_conv2, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(self._n_conv2)\n",
    "        self.do2 = torch.nn.Dropout(self._drop_conv)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(self._n_conv2, self._n_conv3, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(self._n_conv3)\n",
    "        self.do3 = torch.nn.Dropout(self._drop_conv)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(self._flattened_size, self._n_ff)\n",
    "        self.do4 = torch.nn.Dropout(self._drop_dense)\n",
    "        self.fc2 = torch.nn.Linear(self._n_ff, self._n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input size is (batch_size, channels, height, width) = (., 3, 32, 32)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv1(x)                        # (n_conv1, 32, 32)\n",
    "        x = self.bn1(x)                          # same\n",
    "        x = torch.nn.functional.relu(x)          # same\n",
    "        x = self.do1(x)                          # same\n",
    "        x = torch.nn.functional.max_pool2d(x, 2) # (n_conv1, 16, 16)\n",
    "\n",
    "        x = self.conv2(x)                        # (n_conv2, 16, 16)\n",
    "        x = self.bn2(x)                          # same\n",
    "        x = torch.nn.functional.relu(x)          # same\n",
    "        x = self.do2(x)                          # same\n",
    "        x = torch.nn.functional.max_pool2d(x, 2) # (n_conv2, 8, 8)\n",
    "\n",
    "        x = self.conv3(x)                        # (n_conv3, 8, 8)\n",
    "        x = self.bn3(x)                          # same\n",
    "        x = torch.nn.functional.relu(x)          # same\n",
    "        x = self.do3(x)                          # same\n",
    "        x = torch.nn.functional.max_pool2d(x, 2) # (n_conv3, 4, 4)\n",
    "\n",
    "        x = x.view(-1, self._flattened_size) # (flattened_size)\n",
    "        x = self.fc1(x)                      # (n_ff)\n",
    "        x = torch.nn.functional.relu(x)      # same\n",
    "        x = self.do4(x)                      # same\n",
    "\n",
    "        x = self.fc2(x)                      # (n_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "print(\"Model parameters: \", sum(p.numel() for p in SmallNet().parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples  : 45000\n",
      "Validation Samples: 5000\n",
      "Training model...\n",
      "Epoch [1/330]: train=3.9301 val=3.5361 (8.4s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Plot loss as fct of epoch\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlotting curves...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 95\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_dataloader, val_dataloader, optimizer, scheduler, epochs, device)\u001b[0m\n\u001b[0;32m     92\u001b[0m t \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;66;03m# current time\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# train function\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# validate model\u001b[39;00m\n\u001b[0;32m    103\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate_epoch(\n\u001b[0;32m    104\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    105\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[0;32m    106\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    107\u001b[0m )\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# move data and target to device\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# zero the parameter gradients\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = SmallNet()\n",
    "model.to(device)\n",
    "\n",
    "# Loss, optimizer and scheduler\n",
    "n_epochs = 330\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\n",
    "\n",
    "# split train and validation sets\n",
    "val_size = int(0.1 * len(raw_dataset))\n",
    "train_size = len(raw_dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(raw_dataset, [train_size, val_size])\n",
    "\n",
    "# create new dataloaders for training and validation sets\n",
    "BATCH_SIZE = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f'Training Samples  : {len(train_dataset)}')\n",
    "print(f'Validation Samples: {len(val_dataset)}')\n",
    "\n",
    "print('Training model...')\n",
    "train_losses, val_losses = fit(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=n_epochs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Plot loss as fct of epoch\n",
    "print('Plotting curves...')\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training')\n",
    "\n",
    "print('Done!');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChadNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 996376 parameters\n",
      "\u001b[1m\u001b[91mAccuracy on the test set: 50.48%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save the model on a file\n",
    "torch.save(model.state_dict(), 'chad_net.pt')\n",
    "\n",
    "loaded_model = ChadNet()\n",
    "loaded_model.load_state_dict(torch.load('chad_net.pt', weights_only=True))\n",
    "evaluate(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Discussion\n",
    "\n",
    "There are two major constraints for this HW, training time constraint (approximately 1 hour) and model size constraint (1M weights). 1M is not much, for reference I found a model online achieving 90% accuracy on the same dataset with 10M weights.\n",
    "\n",
    "In a nutshell, the timing constraint forces us to have an efficient and compact model structure, otherwise it will take more time to train. We need a compromise in order to maximize final performance. Given the size AND timing constraint, it is not necessarily the largest model which will perform the best.\n",
    "\n",
    "The following list summarizes the major improvements made to the initial model:\n",
    "- initial model has 4% test accuracy (100 classes, so random accuracy is 1%)\n",
    "- increasing batch size -> 35%\n",
    "- adding a conv layer -> 43%\n",
    "- more dense weights -> 48%\n",
    "- step scheduler with 3x epochs -> 52%\n",
    "- maximum number of epochs -> ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "I improved on the base architecture provided mainly by adding one extra convolutional layer, and increasing the neuron count in the first feed-forward dense layer.\n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "The addition of a third convolutional layer yields an improvement in test accuracy of ~7%. This third convolution layer allows the model to pick up on higher level image patterns, so it is able to detect objects more precisely.\n",
    "\n",
    "Every convolution layer is followed by a maximum pooling layer which decreases image size by a factor of 4. However, the convolution layer only increases the amount of channels by 2. This means that compared to the base model, my model has 2x less floats in its hidden layer in between the conv and dense layers (`_n_flattened_size`). The amount of weights in the first FF layer scales quadratically with this hidden layer size, so adding this third convolution layer permits a fair decrease in parameter count.\n",
    "\n",
    "### Dense Layers\n",
    "\n",
    "The previous comment about adding a third conv layer allowed me to increase the number of weights in the first dense layer. It is generally accepted that 2 feed-forward layers at the end of the model is enough for classification, so I chose not to add another dense layer. We can still increase weights in the existing dense layers. Should we?\n",
    "\n",
    "We should choose the dense weight count according to the rest of the architecture. If the model has a small convolutional part, it's useless to give it a huge dense layer, in the same way that it is foolish to have a very big and complex convolutional part followed by very limited/small dense layers. We try to avoid layers \"bottlenecking\" other layers.\n",
    "\n",
    "I performed two 10-epochs trains on two different models:\n",
    "- a model with 200 neurons in the dense HL (~500K weights)\n",
    "- a model with 400 neurons in the dense HL (~1M weights)\n",
    "\n",
    "I found that the largest model performs much better (48% against 42%). Given the model size difference, it's not a huge surprise... This suggests we could have used even more weights in the dense layer, since I obviously chose the dense weight count so that the parameter count falls right under the size constraint.\n",
    "\n",
    "We will rediscuss this in the \"Training Time\" section.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Batch normalization normalizes the output of some layer. This helps the model converge faster and combats vanishing gradient. In theory, normalizing layer outputs permits the use of higher training rates during training as it fixes internal covariate shift.\n",
    "\n",
    "For convolutional neural layers it is usually recommended to put them in between the convolutional layer and the activation layer (ReLU in our case).\n",
    "\n",
    "### Dropout Layers\n",
    "Dropout layers are usually good for regularization in order to avoid overfitting. It looks like the model is overfitting quite fast, since the loss keeps decreasing but the test accuracy does not increase when going from 10 to 30 training epochs.\n",
    "\n",
    "Test accuracy decreases for positive dropout rates for convolutional layers; it is usually preferred to add dropout layers inside dense feed-forward layers so I just removed the dropouts from the convolutional layers.\n",
    "\n",
    "A standard value for dropout is 20-50%. I found that a 20% dropout layer added in between the feed-forward layers yields a 2-3% increase in test accuracy. This most likely helps the model avoid overfitting.\n",
    "\n",
    "We can see on the plotted loss curves that it does reduce overfitting, but it does reduce the test accuracy in the end. Maybe during a longer training time however, this effect. Unfortunately, I did not take the time to do a 1h long training, so I decided to only keep the dropout layer in the dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function & Parameters\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "The first parameter which I changed was batch size. Model performance highly depends on batch size, because larger batch sizes produce gradient descents that are more aligned with the \"true\" gradient. This tends to accelerate convergence. Batch sizes that are way too large tend to overfit training data.\n",
    "\n",
    "A higher batch size also has a nice side effect of speeding up training. I chose a batch size of 128 as my GPU could handle it (faster training), and as the final model performance didn't suffer from increasing it (initial batch size is 32).\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "I tried three different optimizers (Adam, Adaw with weight decay and SGD). SGD is usually the go-to for classification tasks, and yielded slightly better results than the other two.\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "When using 10 epochs for training, any initial learning rate above $10^{-3}$ yields a significant decrease in test accuracy. Lower values seem to have less influence on final test accuracy. This suggests that $10^{-3}$ is a good initial learning rate, and it can then be successively lowered during training.\n",
    "\n",
    "I used a **step learning rate scheduler** which multiplies learning rate by a constant $\\gamma$ every $N_{steps}$ epochs. It permits to have a high learning rate at the beginning of the training phase and gradually decrease it. \n",
    "\n",
    "I found that 20 epochs at constant learning rate of $10^{-3}$ is 3~5% less performant (in test accuracy) than first 10 epochs with a LR of $ 10^{-3} $ and then 10 epochs with a LR of $ 2 \\cdot 10^{-4} $. This can be achieved using a step size of 10 epochs and a $\\gamma$ value of 20%. Increasing epoch number to 30 using the same scheduler yields again an increase in test accuracy of ~2%.\n",
    "\n",
    "The value for $\\gamma$ should be sufficiently large to be meaningful every step size, otherwise the training would be too long. If it's too low, we are missing out on performance. I settled with $\\gamma = 0.2$, changing it so something higher and lower (0.1-0.3) didn't seem to make much of a difference.\n",
    "\n",
    "### Training Time\n",
    "\n",
    "Models with more weights tend to have better performance when sufficiently trained, however they are slower to converge. Smaller models tend to converge faster, but might have a lower performance potential. We have a time constraint now, so we need to determine if it's best to \"heavily train\" a smaller model, or \"lightly train\" a bigger model.\n",
    "\n",
    "Going back to the measurements I did with the dense layer size, I found out that both take almost the same amount of time to train. The largest model takes 20% more time to train even though it has 200% the size. It is likely due to my GPU being able to handle the small size of both models, maybe most of the training time is due to non-computation overheads. 20% training time is worth the 6% accuracy gain, so I'm definitely sticking to the 1M model.\n",
    "\n",
    "### Final Training Time\n",
    "\n",
    "I did my tests on my own laptop equipped with a RTX4060 GPU. Every epoch takes approximately ~10 seconds taking into account validation, a rule of three allows me to estimate the number of epochs I need to train the final model for, so that it does not exist the 1 hour limit. **I will train the final model for 330 epochs**.\n",
    "\n",
    "### Training Function\n",
    "\n",
    "The training function is largely inspired from the `fit` function provided in TP13. I chose to keep the loss function hardcoded to cross entropy to speedup the training by a factor of ~50%. Cross entropy as loss function generally performs good in classification tasks.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "It is easy to see that the model quite rapidly overfits. With no scheduler, the resulting test accuracy is very much the same whether you train the model for 10, 20 or 30 epochs, even though the loss keeps decreasing. This suggests that the model is overfitting the training dataset and not generalizing.\n",
    "\n",
    "For that reason, **I split the train dataset into a train and validation dataset to validate the model at every epoch against unseen data** in order to see how much the model overfits. Ideally, the accuracy against the validation and training dataset should be the same but it's not always the case. I set the validation dataset to 10%. Reducing training dataset size by 10% induced a loss of performance of about 1~2%.\n",
    "\n",
    "This permits two things:\n",
    "- Make sure that the model does not overfit. If it does, we can add more dropouts, etc.\n",
    "- Make sure that the model has learned as much as it could by the end of every step size before decreasing $\\gamma$\n",
    "\n",
    "After plotting validation loss, I realized that a step size of 10 for my step learning rale scheduler was a bit too much because the model tends to overfit above 5 epochs at a given learning rate value. I lowered the step size down to 5.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "One major problem with deep learning is having a high quality dataset. When the dataset is too small, we can generate pseudo-new data by augmenting data. Pytorch allows to virtually increase dataset size by applying data augmenting transformations to the images, like:\n",
    "- random horizontal flips and images crops...\n",
    "- random contrast/saturation/exposition changes...\n",
    "It can also help the model less overfit training data as it generates never-seen data.\n",
    "\n",
    "Data augmentation drastically increases training time. Using only 3 transformations (h-flips, cropping and color jittering) increasing epoch duration by a factor of >5 (from 10-12 to 60-75 seconds). If there was no time constraint, I would have definitely utilized data augmentation because we definitely expect it to increase final model performance.\n",
    "\n",
    "Training a model for 30 epochs did not improve on the test accuracy at all, with respect to the scenario where I didn't use data augmentation, so I decided to ignore data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Possibilities\n",
    "\n",
    "Here are some points I have not explored but could be interesting to further improve on the model performance.\n",
    "\n",
    "### Testing other convolutional/activation layer types\n",
    "Conv2D is not the only convolutional layer structure that exists, maybe some other conv layer structures would be more performant on this classification task. Similarily, maybe other activation functions would yield better performant, for instance a Rectified-Linear-Unit activation function.\n",
    "\n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "Pytorch DataLoader's allow to apply transforms to dataset before using them, it can be used for data augmentation and for data processing. I did not try normalizing the data used for training.\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "I gave a quick try to transfer learning, using resnet18 finetuned on its two last layers.\n",
    "\n",
    "As we saw in TP13, a very common approach to deep learning is to not start from scratch but rather finetune an existing pretrained model onto a specific dataset. It is because patterns learnt during the (sometimes very long) pretraining phase are usually very much interesting in other datasets/use cases. The pretrained model can learn about shapes and objects, and finetuning it using two dense feed-forward layers can have him understand how to classify these shapes/objects.\n",
    "\n",
    "With little effort (transposing TP13 to CIFAR100) I could easily get >25% test accuracy. I eventually settled with training my own model since resnet18 has way too much parameters (18M) compared to the constraint, but transfer learning is definitely something I could have explored more. You can find the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:  11227812\n"
     ]
    }
   ],
   "source": [
    "n_classes = 100\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class FinetuningLastLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # load pretrained resnet18\n",
    "        self.network = resnet18(n_classes)\n",
    "        self.network.to(device)\n",
    "\n",
    "        # freeze the parameters of the network.\n",
    "        for param in self.network.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # replace the last layer with a randonly initializied one and with `num_classes` number of output neurons.\n",
    "        self.network.fc = torch.nn.Linear(self.network.fc.in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# too much parameters!!!\n",
    "print(\"Model parameters: \", sum(p.numel() for p in FinetuningLastLayer().network.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11227812 parameters\n",
      "\u001b[1m\u001b[91mAccuracy on the test set: 25.34%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save the model on a file\n",
    "torch.save(model.state_dict(), 'resnet.pt')\n",
    "\n",
    "loaded_model = FinetuningLastLayer()\n",
    "loaded_model.load_state_dict(torch.load('resnet.pt', weights_only=True))\n",
    "evaluate(loaded_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
