{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pw5yCgwZ7B7"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IJEJb3NAZ7CA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBgennaCZ7CH",
        "outputId": "0a4750b5-c95d-4735-f766-1ec1fd5a361a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import the best device available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyaDvDM8Z7CK"
      },
      "source": [
        "### Choice of dataset\n",
        "\n",
        "The first possible dataset is **permuted MNIST**. The model will be trained on different variations of the same dataset. Each variation corresponds to a different classification task as stated in the paper. The permuted MNIST dataset consists in fist flattening the black and white images of hand written digits and applying a random permutation to the whole dataset of flattened pixel data vectors.\n",
        "\n",
        "The second possible dataset is **rotated MNIST**. Each variation corresponds to MNIST rotated by $ 10(i-1)$ degree if $i$ is the index of the variation/task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CUwrhoIcZ7CM"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "VAL_FRAC   = 0.1\n",
        "DEBUG      = True # Accelerates training by ~90% by only uses 10% of training dataset.\n",
        "                   # Switch off for generating figures, switch on for testing code.\n",
        "\n",
        "def take_subset(dataset, frac):\n",
        "    subset_size = int(frac * len(dataset))\n",
        "    indices = torch.randperm(len(dataset))[:subset_size]\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "def generate_permuted_mnist(i):\n",
        "    \"\"\"\n",
        "    Generates a permuted MNIST dataset with random permutation\n",
        "    \n",
        "    author = Jules\n",
        "    \"\"\"\n",
        "\n",
        "    # Define transform\n",
        "    TRANSFORM = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
        "    ])\n",
        "\n",
        "    # load dataset\n",
        "    raw_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=TRANSFORM, download=True)\n",
        "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=TRANSFORM, download=True)\n",
        "\n",
        "    # Permute dataset splits\n",
        "    permutation = torch.randperm(28 * 28)\n",
        "    raw_dataset.data = raw_dataset.data.view(-1, 28 * 28)[:, permutation].view(-1, 28, 28)\n",
        "    test_dataset.data = test_dataset.data.view(-1, 28 * 28)[:, permutation].view(-1, 28, 28)\n",
        "    \n",
        "    return raw_dataset, test_dataset\n",
        "\n",
        "def generate_rotated_mnist(i):\n",
        "    \"\"\"\n",
        "    Generates a rotated MNIST dataset\n",
        "\n",
        "    author = Pierre\n",
        "    \"\"\"\n",
        "\n",
        "    # Define transform\n",
        "    TRANSFORM = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    # Load datasets\n",
        "    raw_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=TRANSFORM, download=True)\n",
        "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=TRANSFORM, download=True)\n",
        "\n",
        "    # Rotate dataset splits\n",
        "    angle = i * 36 # Define the rotation angle in degrees\n",
        "\n",
        "    def rotate_dataset(dataset, angle):\n",
        "        rotated_data = []\n",
        "        for j in range(len(dataset)):\n",
        "            img, label = dataset[j]\n",
        "            img = transforms.functional.rotate(img, angle) # Rotate image\n",
        "            img = img.view(-1)\n",
        "            rotated_data.append((img, label))\n",
        "        return rotated_data\n",
        "\n",
        "    raw_dataset = rotate_dataset(raw_dataset, angle)\n",
        "    test_dataset = rotate_dataset(test_dataset, angle)\n",
        "\n",
        "    return raw_dataset, test_dataset\n",
        "\n",
        "\n",
        "def generate_tasks(n_tasks, dataset_gen):\n",
        "    \"\"\"\n",
        "    Used to generate N dataloaders corresponding to N different tasks.\n",
        "    Training dataset is split into training and validation.\n",
        "\n",
        "    author = Jules\n",
        "    \"\"\"\n",
        "\n",
        "    raw_datasets = []\n",
        "    test_datasets = []\n",
        "\n",
        "    # Generate datasets\n",
        "    for i_task in range(n_tasks):\n",
        "        raw_dataset, test_dataset = dataset_gen(i_task)\n",
        "\n",
        "        # DEBUG CODE\n",
        "        if DEBUG:\n",
        "            raw_dataset = take_subset(raw_dataset, 0.5)\n",
        "            test_dataset = take_subset(test_dataset, 0.5)\n",
        "\n",
        "        raw_datasets.append(raw_dataset)\n",
        "        test_datasets.append(test_dataset)\n",
        "\n",
        "    train_loaders = []\n",
        "    val_loaders = []\n",
        "    test_loaders = []\n",
        "\n",
        "    # Generate dataloaders\n",
        "    for i_task in range(n_tasks):\n",
        "        raw_dataset = raw_datasets[i_task]\n",
        "        test_dataset = test_datasets[i_task]\n",
        "\n",
        "        # Split train into train and validation\n",
        "        val_size = int(VAL_FRAC * len(raw_dataset))\n",
        "        train_size = len(raw_dataset) - val_size\n",
        "        torch.manual_seed(42)  # make sure epochs have the same validation dataset\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(raw_dataset, [train_size, val_size])\n",
        "\n",
        "        # Turn into data loaders\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        print(f'Task {i_task + 1} Splits: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test')\n",
        "\n",
        "        train_loaders.append(train_loader)\n",
        "        val_loaders.append(val_loader)\n",
        "        test_loaders.append(test_loader)\n",
        "\n",
        "    return train_loaders, val_loaders, test_loaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxwDdZYvZ7CQ"
      },
      "source": [
        "## Util Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVcftF1BZ7CS"
      },
      "source": [
        "### Validation\n",
        "\n",
        "Appendix 4.1 suggests figure 3B was realized using early stopping, which requires the use of a validation split. For this reason, we split the training dataset into a validation and training dataset. Early stopping is as follows in the paper: if validation is seen increasing 5 consecutive times (arbitrary threshold), training is stopped and the model performing the best on previous epochs is kept. This requires to periodically save the dataset and load again the best performing model at the end of the training function.\n",
        "\n",
        "Validation dataset is used to compute the validation loss, however the test set is used to compute the test accuracy (or `Fraction correct` in the paper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u_6JmfltZ7CU"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(\n",
        "    model: torch.nn.Module,\n",
        "    test_dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate test accuracy of the given model on the test split\n",
        "    \"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ujw2tAHHZ7CX"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    val_dataloader: DataLoader,\n",
        "    criterion,\n",
        "    device: torch.device,\n",
        "    early_stopping = False\n",
        "):\n",
        "    \"\"\"\n",
        "    This function validates the model on the validation dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    val_loss = 0.0\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    count_increase = 0\n",
        "    previous_loss = -1\n",
        "    best_model = None\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(val_dataloader):\n",
        "            # move data and target to device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # do the forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            # compute the loss\n",
        "            try:    #Case of EWC\n",
        "                loss = criterion(output, target, model)\n",
        "            except TypeError:\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "            # print statistics\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss / len(val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMDP9uSAZ7Cc"
      },
      "source": [
        "### Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UktMEJbNZ7Cc"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion,\n",
        "    device: torch.device,\n",
        "):\n",
        "    \"\"\"\n",
        "    Training function. Loads the batch, perform forward pass, compute gradients and perform backward pass.\n",
        "    Only difference is that you can pass the loss function as an argument.\n",
        "    This allows to use a simple cross entropy loss function, or a more complex one including L2 or EWC regularization.\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = 0.0\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        # move data and target to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # do the forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # compute the loss\n",
        "        try:    # For EWC\n",
        "            loss = criterion(output, target, model)\n",
        "        except TypeError:\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "        # compute the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # perform the gradient step\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    return train_loss / len(train_dataloader)\n",
        "\n",
        "def fit(\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
        "    criterion,\n",
        "    epochs: int,\n",
        "    device: torch.device,\n",
        "    early_stopping = False\n",
        "):\n",
        "    \"\"\"\n",
        "    The fit method calls the train_epoch() method for a specified\n",
        "    number of epochs and returns the train and validation losses.\n",
        "    \"\"\"\n",
        "\n",
        "    global N_TASKS\n",
        "\n",
        "    # keep track of losses and accuracies\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    count_increase = 0\n",
        "    best_model = None   # For early stopping\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        t = time() # current time\n",
        "\n",
        "        # train function\n",
        "        train_loss = train_epoch(\n",
        "            model=model,\n",
        "            train_dataloader=train_dataloader,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # validate epoch\n",
        "        val_loss = validate_epoch(\n",
        "            model=model,\n",
        "            val_dataloader=val_dataloader,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            early_stopping=early_stopping\n",
        "        )\n",
        "\n",
        "        if early_stopping and len(val_losses) >0 and val_loss > max(val_losses):\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "        if early_stopping and epoch > 0:\n",
        "            if val_loss > val_losses[-1]:\n",
        "                count_increase += 1\n",
        "                if count_increase == 5:\n",
        "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                    # We reset the weights to the best epoch in terms of validation error\n",
        "                    model.load_state_dict(best_model)\n",
        "                    return train_losses, val_losses\n",
        "            else:\n",
        "                count_increase = 0\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # step scheduler if needed\n",
        "        if scheduler != None:\n",
        "            scheduler.step()\n",
        "\n",
        "        dt = time() - t # time difference\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}]: train={round(train_loss, 4)} val={round(val_loss, 4)}, ({round(dt, 2)}s)\")\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVYLa34PZ7Ce"
      },
      "source": [
        "## EWC loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U-2rSqjjZ7Ce"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "class EWC(object):\n",
        "    def __init__(self, model, dataset: list):\n",
        "\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "        self._means = {}\n",
        "        self._precision_matrices = self._diag_fisher()\n",
        "\n",
        "        for n, p in deepcopy(self.params).items():\n",
        "            self._means[n] = p.data\n",
        "\n",
        "    def _diag_fisher(self):\n",
        "        precision_matrices = {}\n",
        "        for n, p in deepcopy(self.params).items():\n",
        "            p.data.zero_()\n",
        "            precision_matrices[n] = p.data\n",
        "\n",
        "        self.model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(self.dataset):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            self.model.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = F.nll_loss(F.log_softmax(output, dim=1), target)\n",
        "            loss.backward()\n",
        "\n",
        "            for n, p in self.model.named_parameters():\n",
        "                precision_matrices[n].data += p.grad.data ** 2 / len(self.dataset)\n",
        "\n",
        "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
        "        return precision_matrices\n",
        "\n",
        "    def penalty(self, new_model):\n",
        "        loss = 0\n",
        "        for n, p in new_model.named_parameters():\n",
        "            _loss = 1/2 *self._precision_matrices[n] * (p - self._means[n]) ** 2\n",
        "            loss += _loss.sum()\n",
        "        return loss\n",
        "\n",
        "def generate_ewc_loss(penalties, lambda_regul):\n",
        "    \"\"\"\n",
        "    Generates an EWC loss function given list of penalties to apply (instances of EWC class)\n",
        "    and value of regularization parameter lambda.\n",
        "    \n",
        "    author = Pierre\n",
        "    \"\"\"\n",
        "\n",
        "    def ewc_loss_function(output, target, new_model):\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        #print(f'raw loss = {loss}')\n",
        "        for penalty in penalties:\n",
        "            loss += lambda_regul * penalty(new_model)\n",
        "            #print(f'   raw penalty = {raw_penalty}')\n",
        "        return loss\n",
        "\n",
        "    return ewc_loss_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHposqgZ7Cf"
      },
      "source": [
        "## Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNmRBcgGZ7Cf",
        "outputId": "4d97eb14-c545-4bb7-c870-6ebb76445a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Parameters:  478410\n"
          ]
        }
      ],
      "source": [
        "class SmallNet(torch.nn.Module):\n",
        "    def __init__(self, width_hidden_layers = 400):\n",
        "        super(SmallNet, self).__init__()\n",
        "\n",
        "        self._input_size = 28 * 28\n",
        "        self._n_ff      = width_hidden_layers\n",
        "        self._n_output   = 10\n",
        "        self._do_input   = 0.2\n",
        "        self._do_ff      = 0.5\n",
        "\n",
        "        self.do0 = torch.nn.Dropout(self._do_input)\n",
        "\n",
        "        self.ff1 = torch.nn.Linear(self._input_size, self._n_ff)\n",
        "        self.do1 = torch.nn.Dropout(self._do_ff)\n",
        "\n",
        "        self.ff2 = torch.nn.Linear(self._n_ff, self._n_ff)\n",
        "        self.do2 = torch.nn.Dropout(self._do_ff)\n",
        "\n",
        "        self.ff7 = torch.nn.Linear(self._n_ff, self._n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # input dropout of 20% as stated in paper\n",
        "        x = self.do0(x)\n",
        "\n",
        "        # dense FF hidden layers, each with 50% dropout and ReLU activation\n",
        "        # dropout is generally placed after the activation\n",
        "        x = self.do1(F.relu(self.ff1(x)))\n",
        "        x = self.do2(F.relu(self.ff2(x)))\n",
        "\n",
        "        # classification layer\n",
        "        x = self.ff7(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"Model Parameters: \", sum(p.numel() for p in SmallNet().parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5ka8MHDxbfp"
      },
      "source": [
        "## Grid Search for learning rate and width of hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM7bKrd5xmC-",
        "outputId": "4aad8ff6-28a5-4fb4-c966-5c232e2c6a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate : 0.001 ; Width layer : 400\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3033 val=2.3009, (0.8s)\n",
            "Epoch [2/10]: train=2.302 val=2.2998, (0.8s)\n",
            "Epoch [3/10]: train=2.3001 val=2.2987, (1.0s)\n",
            "Epoch [4/10]: train=2.2991 val=2.2976, (0.8s)\n",
            "Epoch [5/10]: train=2.2987 val=2.2965, (0.7s)\n",
            "Epoch [6/10]: train=2.298 val=2.2953, (0.6s)\n",
            "Epoch [7/10]: train=2.296 val=2.2942, (0.6s)\n",
            "Epoch [8/10]: train=2.2959 val=2.2931, (0.6s)\n",
            "Epoch [9/10]: train=2.2932 val=2.2919, (0.7s)\n",
            "Epoch [10/10]: train=2.2923 val=2.2908, (0.6s)\n",
            "Learning rate : 0.001 ; Width layer : 577\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.305 val=2.3033, (0.7s)\n",
            "Epoch [2/10]: train=2.3027 val=2.3018, (0.6s)\n",
            "Epoch [3/10]: train=2.3022 val=2.3003, (0.7s)\n",
            "Epoch [4/10]: train=2.301 val=2.2988, (0.7s)\n",
            "Epoch [5/10]: train=2.3002 val=2.2973, (0.6s)\n",
            "Epoch [6/10]: train=2.2962 val=2.2958, (0.6s)\n",
            "Epoch [7/10]: train=2.2959 val=2.2943, (0.6s)\n",
            "Epoch [8/10]: train=2.2948 val=2.2928, (0.6s)\n",
            "Epoch [9/10]: train=2.2929 val=2.2913, (0.9s)\n",
            "Epoch [10/10]: train=2.2929 val=2.2898, (0.9s)\n",
            "Learning rate : 0.001 ; Width layer : 755\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3074 val=2.3042, (0.8s)\n",
            "Epoch [2/10]: train=2.3058 val=2.3024, (0.7s)\n",
            "Epoch [3/10]: train=2.3049 val=2.3006, (0.6s)\n",
            "Epoch [4/10]: train=2.3 val=2.2987, (0.6s)\n",
            "Epoch [5/10]: train=2.3016 val=2.2969, (0.6s)\n",
            "Epoch [6/10]: train=2.2976 val=2.295, (0.6s)\n",
            "Epoch [7/10]: train=2.2968 val=2.2932, (0.6s)\n",
            "Epoch [8/10]: train=2.2941 val=2.2914, (0.7s)\n",
            "Epoch [9/10]: train=2.2942 val=2.2896, (0.6s)\n",
            "Epoch [10/10]: train=2.2914 val=2.2877, (0.6s)\n",
            "Learning rate : 0.001 ; Width layer : 933\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3029 val=2.3018, (0.7s)\n",
            "Epoch [2/10]: train=2.3002 val=2.2994, (0.6s)\n",
            "Epoch [3/10]: train=2.2987 val=2.2969, (0.7s)\n",
            "Epoch [4/10]: train=2.2975 val=2.2945, (0.6s)\n",
            "Epoch [5/10]: train=2.2936 val=2.2921, (0.6s)\n",
            "Epoch [6/10]: train=2.2914 val=2.2897, (0.9s)\n",
            "Epoch [7/10]: train=2.2902 val=2.2873, (0.9s)\n",
            "Epoch [8/10]: train=2.2874 val=2.2848, (0.9s)\n",
            "Epoch [9/10]: train=2.2871 val=2.2824, (0.6s)\n",
            "Epoch [10/10]: train=2.2853 val=2.28, (0.7s)\n",
            "Learning rate : 0.001 ; Width layer : 1111\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3058 val=2.3027, (0.7s)\n",
            "Epoch [2/10]: train=2.304 val=2.3001, (0.7s)\n",
            "Epoch [3/10]: train=2.3013 val=2.2976, (0.7s)\n",
            "Epoch [4/10]: train=2.2989 val=2.295, (0.6s)\n",
            "Epoch [5/10]: train=2.296 val=2.2924, (0.6s)\n",
            "Epoch [6/10]: train=2.2956 val=2.2899, (0.7s)\n",
            "Epoch [7/10]: train=2.2919 val=2.2873, (0.6s)\n",
            "Epoch [8/10]: train=2.2917 val=2.2848, (0.6s)\n",
            "Epoch [9/10]: train=2.2881 val=2.2822, (0.7s)\n",
            "Epoch [10/10]: train=2.2857 val=2.2797, (0.6s)\n",
            "Learning rate : 0.001 ; Width layer : 1288\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3024 val=2.297, (0.7s)\n",
            "Epoch [2/10]: train=2.2992 val=2.2945, (0.8s)\n",
            "Epoch [3/10]: train=2.2964 val=2.2919, (0.8s)\n",
            "Epoch [4/10]: train=2.2944 val=2.2894, (1.0s)\n",
            "Epoch [5/10]: train=2.2908 val=2.2868, (0.8s)\n",
            "Epoch [6/10]: train=2.2877 val=2.2843, (0.7s)\n",
            "Epoch [7/10]: train=2.2864 val=2.2817, (0.7s)\n",
            "Epoch [8/10]: train=2.2843 val=2.2792, (0.6s)\n",
            "Epoch [9/10]: train=2.282 val=2.2766, (0.7s)\n",
            "Epoch [10/10]: train=2.2783 val=2.2741, (0.7s)\n",
            "Learning rate : 0.001 ; Width layer : 1466\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3054 val=2.3054, (0.7s)\n",
            "Epoch [2/10]: train=2.3022 val=2.3026, (0.6s)\n",
            "Epoch [3/10]: train=2.2986 val=2.2998, (0.7s)\n",
            "Epoch [4/10]: train=2.2966 val=2.297, (0.7s)\n",
            "Epoch [5/10]: train=2.2921 val=2.2943, (0.7s)\n",
            "Epoch [6/10]: train=2.2916 val=2.2915, (0.9s)\n",
            "Epoch [7/10]: train=2.2901 val=2.2888, (1.0s)\n",
            "Epoch [8/10]: train=2.2852 val=2.2861, (0.8s)\n",
            "Epoch [9/10]: train=2.2821 val=2.2833, (0.9s)\n",
            "Epoch [10/10]: train=2.2782 val=2.2806, (1.0s)\n",
            "Learning rate : 0.001 ; Width layer : 1644\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3072 val=2.3029, (0.7s)\n",
            "Epoch [2/10]: train=2.3033 val=2.2996, (0.7s)\n",
            "Epoch [3/10]: train=2.2986 val=2.2962, (0.7s)\n",
            "Epoch [4/10]: train=2.2961 val=2.2929, (0.7s)\n",
            "Epoch [5/10]: train=2.2942 val=2.2896, (0.7s)\n",
            "Epoch [6/10]: train=2.2902 val=2.2864, (0.7s)\n",
            "Epoch [7/10]: train=2.2867 val=2.2831, (0.7s)\n",
            "Epoch [8/10]: train=2.2837 val=2.2799, (0.7s)\n",
            "Epoch [9/10]: train=2.2802 val=2.2766, (0.7s)\n",
            "Epoch [10/10]: train=2.2766 val=2.2734, (0.7s)\n",
            "Learning rate : 0.001 ; Width layer : 1822\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3057 val=2.3033, (0.7s)\n",
            "Epoch [2/10]: train=2.3053 val=2.2998, (0.7s)\n",
            "Epoch [3/10]: train=2.3003 val=2.2963, (0.7s)\n",
            "Epoch [4/10]: train=2.2976 val=2.2929, (0.7s)\n",
            "Epoch [5/10]: train=2.2947 val=2.2894, (0.9s)\n",
            "Epoch [6/10]: train=2.2915 val=2.286, (0.9s)\n",
            "Epoch [7/10]: train=2.2881 val=2.2825, (1.1s)\n",
            "Epoch [8/10]: train=2.2858 val=2.279, (0.7s)\n",
            "Epoch [9/10]: train=2.2813 val=2.2756, (0.7s)\n",
            "Epoch [10/10]: train=2.2789 val=2.2721, (0.7s)\n",
            "Learning rate : 0.001 ; Width layer : 2000\n",
            "Task 1 Splits: 5400 train, 600 val, 1000 test\n",
            "Epoch [1/10]: train=2.3037 val=2.3021, (0.7s)\n",
            "Epoch [2/10]: train=2.2999 val=2.2983, (0.7s)\n",
            "Epoch [3/10]: train=2.2978 val=2.2946, (0.7s)\n",
            "Epoch [4/10]: train=2.2935 val=2.2909, (0.7s)\n",
            "Epoch [5/10]: train=2.2916 val=2.2872, (0.7s)\n",
            "Epoch [6/10]: train=2.2875 val=2.2835, (0.7s)\n",
            "Epoch [7/10]: train=2.2836 val=2.2799, (0.7s)\n",
            "Epoch [8/10]: train=2.2809 val=2.2762, (0.7s)\n",
            "Epoch [9/10]: train=2.2773 val=2.2725, (0.7s)\n",
            "Epoch [10/10]: train=2.2754 val=2.2688, (0.7s)\n",
            "Best parameters: {'width_hidden_layer': 2000}\n",
            "Best validation loss: 2.2688498497009277\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "N_EPOCHS = 10\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    #'lr': np.logspace(-5, -3, 10),\n",
        "    'width_hidden_layer': np.linspace(400, 2000, 10).astype(int)\n",
        "}\n",
        "\n",
        "width_hidden_layer = 800\n",
        "lr=0.001\n",
        "# Initialize variables to store the best parameters and the best validation loss\n",
        "best_params = None\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Perform grid search\n",
        "for params in ParameterGrid(param_grid):\n",
        "\n",
        "    #lr = params['lr']\n",
        "    width_hidden_layer = params['width_hidden_layer']\n",
        "    print(\"Learning rate :\", lr, '; Width layer :', width_hidden_layer)\n",
        "    # Initialize model with the current width_hidden_layer\n",
        "    model = SmallNet(width_hidden_layers=width_hidden_layer)\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer with the current learning rate\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = F.cross_entropy\n",
        "\n",
        "    # Train the model\n",
        "    train_dataloader, val_dataloader, _ = generate_tasks(1, generate_permuted_mnist)\n",
        "    train_losses, val_losses = fit(\n",
        "        model=model,\n",
        "        train_dataloader=train_dataloader[0],\n",
        "        val_dataloader=val_dataloader[0],\n",
        "        optimizer=optimizer,\n",
        "        scheduler=None,\n",
        "        criterion=criterion,\n",
        "        epochs=N_EPOCHS,\n",
        "        device=device,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Get the validation loss of the last epoch\n",
        "    val_loss = val_losses[-1]\n",
        "\n",
        "    # Update the best parameters if the current validation loss is lower\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_params = params\n",
        "\n",
        "print(f'Best parameters: {best_params}')\n",
        "print(f'Best validation loss: {best_val_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp26C908Z7Ch"
      },
      "source": [
        "## Training Script\n",
        "\n",
        "The training script generates $N$ different tasks and calls the `fit(..)` function as many times on different tasks. It then saves the curves and model obtained. It has to be ran multiple times with different loss functions to generate subfigure 2A\n",
        "\n",
        "No scheduler seems to have been used in the paper, so we are not using any here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task 1 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 2 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 3 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 4 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 5 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 6 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 7 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 8 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 9 Splits: 27000 train, 3000 val, 5000 test\n",
            "Task 10 Splits: 27000 train, 3000 val, 5000 test\n"
          ]
        }
      ],
      "source": [
        "#################################################################################\n",
        "# N_EPOCHS    = Number of epochs per task.\n",
        "# N_TASKS     = Number of tasks to train the model on.\n",
        "# DATASET_GEN = Type of tasks. We can choose between permuted MNIST and rotated MNIST\n",
        "# REGUL_TYPE  = Type of regularization. Very important for the paper, as it changes the training process.\n",
        "#               - `SGD` | Classic SGD with no regularization term\n",
        "#               - `L2`  | SGD with L2 regularization term\n",
        "#               - `EWC` | SGD with elastic weight consolidation regularization\n",
        "#################################################################################\n",
        "N_TASKS     = 10\n",
        "DATASET_GEN = generate_permuted_mnist\n",
        "\n",
        "train_dataloaders, val_dataloaders, test_dataloaders = generate_tasks(N_TASKS, DATASET_GEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6EkEwTlZ7Ch",
        "outputId": "96a92323-6dbc-40ae-ba5a-cf6470eb72d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================\n",
            "EWC\n",
            "Training on task 1...\n",
            "Epoch [1/30]: train=2.2985 val=2.2856, (5.99s)\n",
            "Epoch [2/30]: train=2.2809 val=2.2667, (5.38s)\n",
            "Epoch [3/30]: train=2.2634 val=2.2477, (4.13s)\n",
            "Epoch [4/30]: train=2.2456 val=2.2278, (5.8s)\n",
            "Epoch [5/30]: train=2.2281 val=2.2069, (6.94s)\n",
            "Epoch [6/30]: train=2.2085 val=2.1844, (6.43s)\n",
            "Epoch [7/30]: train=2.1866 val=2.1598, (5.94s)\n",
            "Epoch [8/30]: train=2.1641 val=2.1328, (5.04s)\n",
            "Epoch [9/30]: train=2.1403 val=2.1029, (5.29s)\n",
            "Epoch [10/30]: train=2.1119 val=2.0694, (5.6s)\n",
            "Epoch [11/30]: train=2.0816 val=2.0321, (5.72s)\n",
            "Epoch [12/30]: train=2.0475 val=1.9904, (5.78s)\n",
            "Epoch [13/30]: train=2.0071 val=1.9434, (5.37s)\n",
            "Epoch [14/30]: train=1.9642 val=1.8912, (5.37s)\n",
            "Epoch [15/30]: train=1.9148 val=1.8335, (5.93s)\n",
            "Epoch [16/30]: train=1.8596 val=1.7702, (5.87s)\n",
            "Epoch [17/30]: train=1.7997 val=1.7018, (5.81s)\n",
            "Epoch [18/30]: train=1.7371 val=1.6293, (5.77s)\n",
            "Epoch [19/30]: train=1.6662 val=1.5533, (5.14s)\n",
            "Epoch [20/30]: train=1.5964 val=1.4759, (5.79s)\n",
            "Epoch [21/30]: train=1.5211 val=1.3984, (6.03s)\n",
            "Epoch [22/30]: train=1.4483 val=1.3226, (6.07s)\n",
            "Epoch [23/30]: train=1.3789 val=1.2503, (5.85s)\n",
            "Epoch [24/30]: train=1.3106 val=1.182, (5.47s)\n",
            "Epoch [25/30]: train=1.2479 val=1.1189, (5.53s)\n",
            "Epoch [26/30]: train=1.1902 val=1.0612, (5.99s)\n",
            "Epoch [27/30]: train=1.1347 val=1.0085, (5.51s)\n",
            "Epoch [28/30]: train=1.082 val=0.9608, (6.06s)\n",
            "Epoch [29/30]: train=1.0396 val=0.918, (6.13s)\n",
            "Epoch [30/30]: train=0.9971 val=0.8799, (5.21s)\n",
            "[]\n",
            "Training on task 2...\n",
            "Epoch [1/30]: train=2.2724 val=2.12, (6.0s)\n",
            "Epoch [2/30]: train=2.0896 val=1.9367, (6.03s)\n",
            "Epoch [3/30]: train=1.936 val=1.7774, (6.12s)\n",
            "Epoch [4/30]: train=1.7992 val=1.6356, (5.58s)\n",
            "Epoch [5/30]: train=1.6734 val=1.5082, (5.73s)\n",
            "Epoch [6/30]: train=1.5634 val=1.3961, (6.05s)\n",
            "Epoch [7/30]: train=1.4649 val=1.2968, (6.13s)\n",
            "Epoch [8/30]: train=1.375 val=1.2099, (6.3s)\n",
            "Epoch [9/30]: train=1.2955 val=1.1336, (6.12s)\n",
            "Epoch [10/30]: train=1.228 val=1.0664, (5.49s)\n",
            "Epoch [11/30]: train=1.1644 val=1.0081, (5.9s)\n",
            "Epoch [12/30]: train=1.1126 val=0.9566, (6.12s)\n",
            "Epoch [13/30]: train=1.0618 val=0.9112, (6.0s)\n",
            "Epoch [14/30]: train=1.0209 val=0.8714, (5.95s)\n",
            "Epoch [15/30]: train=0.9794 val=0.8356, (5.14s)\n",
            "Epoch [16/30]: train=0.9487 val=0.8042, (5.52s)\n",
            "Epoch [17/30]: train=0.92 val=0.7748, (5.81s)\n",
            "Epoch [18/30]: train=0.8903 val=0.7496, (5.89s)\n",
            "Epoch [19/30]: train=0.8661 val=0.7266, (5.89s)\n",
            "Epoch [20/30]: train=0.8425 val=0.7048, (5.3s)\n",
            "Epoch [21/30]: train=0.8241 val=0.6857, (5.51s)\n",
            "Epoch [22/30]: train=0.8048 val=0.6676, (5.56s)\n",
            "Epoch [23/30]: train=0.7832 val=0.651, (5.6s)\n",
            "Epoch [24/30]: train=0.7687 val=0.6364, (5.82s)\n",
            "Epoch [25/30]: train=0.7533 val=0.6221, (6.04s)\n",
            "Epoch [26/30]: train=0.7402 val=0.6089, (5.23s)\n",
            "Epoch [27/30]: train=0.7262 val=0.5967, (5.44s)\n",
            "Epoch [28/30]: train=0.7122 val=0.5852, (6.05s)\n",
            "Epoch [29/30]: train=0.7033 val=0.5747, (6.05s)\n",
            "Epoch [30/30]: train=0.6934 val=0.5651, (5.19s)\n",
            "[0.8277]\n",
            "Training on task 3...\n",
            "Epoch [1/30]: train=2.085 val=1.6977, (6.31s)\n",
            "Epoch [2/30]: train=1.7174 val=1.422, (5.97s)\n",
            "Epoch [3/30]: train=1.5047 val=1.2428, (5.54s)\n",
            "Epoch [4/30]: train=1.3529 val=1.1172, (6.06s)\n",
            "Epoch [5/30]: train=1.241 val=1.022, (5.39s)\n",
            "Epoch [6/30]: train=1.1573 val=0.9481, (5.84s)\n",
            "Epoch [7/30]: train=1.0871 val=0.8891, (6.03s)\n",
            "Epoch [8/30]: train=1.0283 val=0.8409, (5.77s)\n",
            "Epoch [9/30]: train=0.9851 val=0.8009, (6.22s)\n",
            "Epoch [10/30]: train=0.9417 val=0.7656, (5.49s)\n",
            "Epoch [11/30]: train=0.9064 val=0.7364, (6.12s)\n",
            "Epoch [12/30]: train=0.8766 val=0.7103, (6.18s)\n",
            "Epoch [13/30]: train=0.8558 val=0.6877, (6.2s)\n",
            "Epoch [14/30]: train=0.8312 val=0.6678, (6.22s)\n",
            "Epoch [15/30]: train=0.8147 val=0.6496, (5.48s)\n",
            "Epoch [16/30]: train=0.7879 val=0.6332, (5.69s)\n",
            "Epoch [17/30]: train=0.7723 val=0.6178, (6.05s)\n",
            "Epoch [18/30]: train=0.7576 val=0.6046, (5.87s)\n",
            "Epoch [19/30]: train=0.7455 val=0.5922, (6.04s)\n",
            "Epoch [20/30]: train=0.7296 val=0.5806, (6.15s)\n",
            "Epoch [21/30]: train=0.7136 val=0.5703, (5.65s)\n",
            "Epoch [22/30]: train=0.7054 val=0.5608, (6.12s)\n",
            "Epoch [23/30]: train=0.6923 val=0.5514, (5.73s)\n",
            "Epoch [24/30]: train=0.6864 val=0.5428, (5.87s)\n",
            "Epoch [25/30]: train=0.6768 val=0.535, (5.86s)\n",
            "Epoch [26/30]: train=0.6656 val=0.5278, (5.23s)\n",
            "Epoch [27/30]: train=0.6549 val=0.5208, (5.97s)\n",
            "Epoch [28/30]: train=0.6479 val=0.5144, (4.88s)\n",
            "Epoch [29/30]: train=0.6459 val=0.5078, (9.51s)\n",
            "Epoch [30/30]: train=0.6367 val=0.5018, (6.63s)\n",
            "[0.8277, 0.8167333333333333]\n",
            "Training on task 4...\n",
            "Epoch [1/30]: train=2.258 val=1.6428, (6.68s)\n",
            "Epoch [2/30]: train=1.6655 val=1.2848, (6.23s)\n",
            "Epoch [3/30]: train=1.4028 val=1.0946, (5.46s)\n",
            "Epoch [4/30]: train=1.2392 val=0.9745, (6.34s)\n",
            "Epoch [5/30]: train=1.1355 val=0.8927, (5.84s)\n",
            "Epoch [6/30]: train=1.0589 val=0.8316, (5.6s)\n",
            "Epoch [7/30]: train=0.9971 val=0.7842, (6.64s)\n",
            "Epoch [8/30]: train=0.95 val=0.7464, (5.53s)\n",
            "Epoch [9/30]: train=0.9154 val=0.7154, (5.74s)\n",
            "Epoch [10/30]: train=0.8738 val=0.6895, (6.1s)\n",
            "Epoch [11/30]: train=0.8573 val=0.6671, (6.41s)\n",
            "Epoch [12/30]: train=0.8267 val=0.6476, (5.7s)\n",
            "Epoch [13/30]: train=0.8092 val=0.6305, (5.71s)\n",
            "Epoch [14/30]: train=0.7877 val=0.6158, (5.69s)\n",
            "Epoch [15/30]: train=0.7731 val=0.6024, (6.03s)\n",
            "Epoch [16/30]: train=0.7546 val=0.5899, (5.45s)\n",
            "Epoch [17/30]: train=0.7489 val=0.5787, (5.91s)\n",
            "Epoch [18/30]: train=0.7363 val=0.5689, (6.18s)\n",
            "Epoch [19/30]: train=0.7259 val=0.5598, (6.97s)\n",
            "Epoch [20/30]: train=0.7124 val=0.5515, (13.75s)\n",
            "Epoch [21/30]: train=0.7011 val=0.5433, (13.68s)\n",
            "Epoch [22/30]: train=0.6951 val=0.5359, (13.1s)\n",
            "Epoch [23/30]: train=0.6779 val=0.5292, (11.95s)\n",
            "Epoch [24/30]: train=0.6773 val=0.5226, (12.09s)\n",
            "Epoch [25/30]: train=0.6677 val=0.5166, (11.99s)\n",
            "Epoch [26/30]: train=0.6658 val=0.511, (11.94s)\n",
            "Epoch [27/30]: train=0.6559 val=0.506, (12.15s)\n",
            "Epoch [28/30]: train=0.6509 val=0.5009, (11.77s)\n",
            "Epoch [29/30]: train=0.6462 val=0.4961, (12.09s)\n",
            "Epoch [30/30]: train=0.638 val=0.4914, (11.83s)\n",
            "[0.8277, 0.8167333333333333, 0.80805]\n",
            "Training on task 5...\n",
            "Epoch [1/30]: train=2.2522 val=1.5356, (6.77s)\n",
            "Epoch [2/30]: train=1.5999 val=1.2024, (6.18s)\n",
            "Epoch [3/30]: train=1.3408 val=1.038, (5.98s)\n",
            "Epoch [4/30]: train=1.1957 val=0.9369, (6.5s)\n",
            "Epoch [5/30]: train=1.1023 val=0.8678, (6.31s)\n",
            "Epoch [6/30]: train=1.0309 val=0.8174, (6.41s)\n",
            "Epoch [7/30]: train=0.9831 val=0.778, (6.52s)\n",
            "Epoch [8/30]: train=0.9421 val=0.7468, (5.75s)\n",
            "Epoch [9/30]: train=0.9013 val=0.7202, (5.76s)\n",
            "Epoch [10/30]: train=0.8762 val=0.6992, (6.75s)\n",
            "Epoch [11/30]: train=0.8501 val=0.6795, (6.45s)\n",
            "Epoch [12/30]: train=0.8295 val=0.6629, (6.1s)\n",
            "Epoch [13/30]: train=0.8125 val=0.6478, (5.91s)\n",
            "Epoch [14/30]: train=0.7915 val=0.6357, (6.51s)\n",
            "Epoch [15/30]: train=0.7816 val=0.6235, (6.21s)\n",
            "Epoch [16/30]: train=0.7657 val=0.6134, (5.65s)\n",
            "Epoch [17/30]: train=0.7538 val=0.6034, (6.13s)\n",
            "Epoch [18/30]: train=0.7426 val=0.5946, (13.55s)\n",
            "Epoch [19/30]: train=0.7291 val=0.5864, (9.45s)\n",
            "Epoch [20/30]: train=0.7191 val=0.579, (5.31s)\n",
            "Epoch [21/30]: train=0.7127 val=0.5717, (5.48s)\n",
            "Epoch [22/30]: train=0.703 val=0.565, (5.82s)\n",
            "Epoch [23/30]: train=0.699 val=0.5587, (5.84s)\n",
            "Epoch [24/30]: train=0.693 val=0.5534, (5.76s)\n",
            "Epoch [25/30]: train=0.6883 val=0.5479, (5.92s)\n",
            "Epoch [26/30]: train=0.6751 val=0.5427, (5.79s)\n",
            "Epoch [27/30]: train=0.6687 val=0.5383, (5.49s)\n",
            "Epoch [28/30]: train=0.6602 val=0.534, (5.82s)\n",
            "Epoch [29/30]: train=0.655 val=0.5291, (5.6s)\n",
            "Epoch [30/30]: train=0.6485 val=0.5243, (5.85s)\n",
            "[0.8277, 0.8167333333333333, 0.80805, 0.81436]\n",
            "Training on task 6...\n",
            "Epoch [1/30]: train=2.2436 val=1.5512, (6.38s)\n",
            "Epoch [2/30]: train=1.6374 val=1.2057, (6.22s)\n",
            "Epoch [3/30]: train=1.3709 val=1.031, (6.03s)\n",
            "Epoch [4/30]: train=1.2185 val=0.9248, (6.0s)\n",
            "Epoch [5/30]: train=1.1129 val=0.8537, (6.44s)\n",
            "Epoch [6/30]: train=1.0435 val=0.8037, (6.24s)\n",
            "Epoch [7/30]: train=0.985 val=0.7647, (6.14s)\n",
            "Epoch [8/30]: train=0.945 val=0.7335, (6.34s)\n",
            "Epoch [9/30]: train=0.9021 val=0.7084, (6.17s)\n",
            "Epoch [10/30]: train=0.8692 val=0.6879, (6.18s)\n",
            "Epoch [11/30]: train=0.8498 val=0.6693, (5.93s)\n",
            "Epoch [12/30]: train=0.8329 val=0.6532, (6.32s)\n",
            "Epoch [13/30]: train=0.813 val=0.6401, (6.12s)\n",
            "Epoch [14/30]: train=0.7935 val=0.6272, (5.93s)\n",
            "Epoch [15/30]: train=0.7828 val=0.6168, (6.1s)\n",
            "Epoch [16/30]: train=0.7659 val=0.6066, (6.25s)\n",
            "Epoch [17/30]: train=0.7573 val=0.5983, (6.24s)\n",
            "Epoch [18/30]: train=0.7421 val=0.5903, (6.17s)\n",
            "Epoch [19/30]: train=0.7328 val=0.5817, (6.14s)\n",
            "Epoch [20/30]: train=0.7221 val=0.5748, (6.22s)\n",
            "Epoch [21/30]: train=0.7135 val=0.5683, (6.12s)\n",
            "Epoch [22/30]: train=0.7044 val=0.5624, (5.97s)\n",
            "Epoch [23/30]: train=0.7037 val=0.5565, (6.37s)\n",
            "Epoch [24/30]: train=0.6925 val=0.5507, (6.07s)\n",
            "Epoch [25/30]: train=0.6852 val=0.5459, (11.61s)\n",
            "Epoch [26/30]: train=0.6774 val=0.5414, (9.19s)\n",
            "Epoch [27/30]: train=0.6729 val=0.5368, (6.19s)\n",
            "Epoch [28/30]: train=0.6643 val=0.5321, (6.17s)\n",
            "Epoch [29/30]: train=0.66 val=0.5285, (5.91s)\n",
            "Epoch [30/30]: train=0.6551 val=0.5245, (6.2s)\n",
            "[0.8277, 0.8167333333333333, 0.80805, 0.81436, 0.8154333333333333]\n",
            "Training on task 7...\n",
            "Epoch [1/30]: train=2.2704 val=1.4966, (6.76s)\n",
            "Epoch [2/30]: train=1.6259 val=1.1859, (6.61s)\n",
            "Epoch [3/30]: train=1.3696 val=1.0267, (6.65s)\n",
            "Epoch [4/30]: train=1.2289 val=0.9283, (6.37s)\n",
            "Epoch [5/30]: train=1.1332 val=0.8615, (6.34s)\n",
            "Epoch [6/30]: train=1.0635 val=0.8132, (6.53s)\n",
            "Epoch [7/30]: train=1.0095 val=0.7759, (6.6s)\n",
            "Epoch [8/30]: train=0.9643 val=0.7457, (6.24s)\n",
            "Epoch [9/30]: train=0.934 val=0.7217, (6.74s)\n",
            "Epoch [10/30]: train=0.9075 val=0.6999, (6.81s)\n",
            "Epoch [11/30]: train=0.8766 val=0.6829, (6.54s)\n",
            "Epoch [12/30]: train=0.8627 val=0.6671, (6.81s)\n",
            "Epoch [13/30]: train=0.8357 val=0.6539, (11.04s)\n",
            "Epoch [14/30]: train=0.8234 val=0.6416, (6.58s)\n",
            "Epoch [15/30]: train=0.8102 val=0.6307, (6.76s)\n",
            "Epoch [16/30]: train=0.7955 val=0.6211, (6.5s)\n",
            "Epoch [17/30]: train=0.7872 val=0.6117, (6.5s)\n",
            "Epoch [18/30]: train=0.7666 val=0.6042, (7.01s)\n",
            "Epoch [19/30]: train=0.7656 val=0.5954, (7.02s)\n",
            "Epoch [20/30]: train=0.7504 val=0.5889, (6.76s)\n",
            "Epoch [21/30]: train=0.7476 val=0.5825, (6.36s)\n",
            "Epoch [22/30]: train=0.7405 val=0.5762, (6.32s)\n",
            "Epoch [23/30]: train=0.7292 val=0.5705, (6.69s)\n",
            "Epoch [24/30]: train=0.7217 val=0.5654, (6.69s)\n",
            "Epoch [25/30]: train=0.7156 val=0.5602, (7.26s)\n",
            "Epoch [26/30]: train=0.7107 val=0.5546, (6.29s)\n",
            "Epoch [27/30]: train=0.7005 val=0.55, (6.48s)\n",
            "Epoch [28/30]: train=0.6947 val=0.5456, (6.51s)\n",
            "Epoch [29/30]: train=0.6889 val=0.5416, (6.73s)\n",
            "Epoch [30/30]: train=0.6858 val=0.5375, (6.55s)\n",
            "[0.8277, 0.8167333333333333, 0.80805, 0.81436, 0.8154333333333333, 0.8146571428571429]\n",
            "Training on task 8...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining on task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi_task\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m REGUL_TYPE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEWC\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m     penalties_ewc\u001b[38;5;241m.\u001b[39mappend(\u001b[43mEWC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpenalty) \u001b[38;5;66;03m# Add one regularization per task, as proposed by Kirkpatrick et al.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m generate_ewc_loss(penalties_ewc, \u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# Need to redefine the loss function with the fisher of the former task\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Fit on task\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mEWC.__init__\u001b[1;34m(self, model, dataset)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m {n: p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad}\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_means \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_precision_matrices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_diag_fisher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_means[n] \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mdata\n",
            "Cell \u001b[1;32mIn[7], line 23\u001b[0m, in \u001b[0;36mEWC._diag_fisher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m     precision_matrices[n] \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 23\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Jules\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 30\n",
        "\n",
        "for REGUL_TYPE in [ \"EWC\"]:\n",
        "    print('================================================')\n",
        "    print(REGUL_TYPE)\n",
        "    \n",
        "    # Initialize model\n",
        "    model = SmallNet(width_hidden_layers=2000)\n",
        "    model.to(device)\n",
        "\n",
        "    # Loss, optimizer and scheduler\n",
        "    if REGUL_TYPE == 'SGD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "        criterion = F.cross_entropy\n",
        "    elif REGUL_TYPE == 'EWC':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    else:\n",
        "        raise Exception('Invalid regularization')\n",
        "\n",
        "    full_test_accuracies = []\n",
        "    penalties_ewc = []\n",
        "    for i_task in range(N_TASKS):\n",
        "\n",
        "        # Get tasks\n",
        "        train_dataloader = train_dataloaders[i_task]\n",
        "        val_dataloader = val_dataloaders[i_task]\n",
        "\n",
        "        print(f'Training on task {i_task + 1}...')\n",
        "\n",
        "        if REGUL_TYPE == 'EWC':\n",
        "            penalties_ewc.append(EWC(model, dataset=train_dataloader).penalty) # Add one regularization per task, as proposed by Kirkpatrick et al.\n",
        "            criterion = generate_ewc_loss(penalties_ewc, 100) # Need to redefine the loss function with the fisher of the former task\n",
        "\n",
        "        # Fit on task\n",
        "        train_losses, val_losses = fit(\n",
        "            model=model,\n",
        "            train_dataloader=train_dataloader,\n",
        "            val_dataloader=val_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=None,\n",
        "            criterion=criterion,\n",
        "            epochs=N_EPOCHS,\n",
        "            device=device,\n",
        "            early_stopping=True,\n",
        "        )\n",
        "\n",
        "        # After training on n_tasks, we evaluate the model on all the tasks it learnt.\n",
        "        if i_task >= 1:  # At least two tasks learnt\n",
        "            test_dataloader = DataLoader(torch.utils.data.ConcatDataset([test_dataloaders[j].dataset for j in range(i_task + 1)]))\n",
        "            full_test_accuracies.append(evaluate_accuracy(model, test_dataloader, device))\n",
        "\n",
        "        print( full_test_accuracies)\n",
        "\n",
        "    # Save model and metrics\n",
        "    np.save(f'2b_{REGUL_TYPE}_test_accuracy.npy', np.array(full_test_accuracies))\n",
        "\n",
        "print('Done!');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g67RcIvZ7Cj"
      },
      "source": [
        "## Generate Figure 2B\n",
        "\n",
        "Follow these steps to generate figure 2B. You can then redo all these steps after changing the dataset from permuted to rotated MNIST using the option `DATASET_GEN`.\n",
        "The regularization technique used can be chosen by changing the option `REGUL_TYPE`. Each reg technique will save their respective train accuracy curves to different files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "93jO6ImkZ7Cj",
        "outputId": "98a9d101-7c66-4703-89e6-5283a2a86665"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "FILE_NAMES = [\n",
        "    'SGD',\n",
        "    'EWC',\n",
        "]\n",
        "color_dict = {'SGD':\"blue\",\n",
        "              #'SGD_one_task': 'black',\n",
        "              'EWC':'red'}\n",
        "legend_dict = {'SGD':'SGD + Dropout',\n",
        "                #'SGD_one_task': 'Single task performance',\n",
        "                'EWC':'EWC'}\n",
        "\n",
        "for file_name in FILE_NAMES:\n",
        "    test_accuracy = np.load(f'2b_{file_name}_test_accuracy.npy')\n",
        "    plt.plot(range(2, N_TASKS +1), test_accuracy, label=legend_dict[file_name], color = color_dict[file_name], alpha = 0.8, marker = 'o')\n",
        "\n",
        "sgd_single_task_perf = np.load(f'2b_SGD_test_accuracy.npy')[0]\n",
        "\n",
        "plt.axhline(y=sgd_single_task_perf, color='g', linestyle='--', label='SGD (Single Task)')\n",
        "\n",
        "plt.ylabel('Frac. correct')\n",
        "plt.xlabel('Number of tasks')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
